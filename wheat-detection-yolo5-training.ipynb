{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nfrom tqdm import tqdm\nimport shutil\n\nfrom sklearn.model_selection import StratifiedKFold","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-09T07:26:19.615731Z","iopub.execute_input":"2023-09-09T07:26:19.616335Z","iopub.status.idle":"2023-09-09T07:26:20.653122Z","shell.execute_reply.started":"2023-09-09T07:26:19.616291Z","shell.execute_reply":"2023-09-09T07:26:20.652147Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/global-wheat-detection/train.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-09T07:26:20.658854Z","iopub.execute_input":"2023-09-09T07:26:20.661703Z","iopub.status.idle":"2023-09-09T07:26:20.953787Z","shell.execute_reply.started":"2023-09-09T07:26:20.661664Z","shell.execute_reply":"2023-09-09T07:26:20.952928Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"    image_id  width  height                         bbox   source\n0  b6ab77fd7   1024    1024   [834.0, 222.0, 56.0, 36.0]  usask_1\n1  b6ab77fd7   1024    1024  [226.0, 548.0, 130.0, 58.0]  usask_1\n2  b6ab77fd7   1024    1024  [377.0, 504.0, 74.0, 160.0]  usask_1\n3  b6ab77fd7   1024    1024  [834.0, 95.0, 109.0, 107.0]  usask_1\n4  b6ab77fd7   1024    1024  [26.0, 144.0, 124.0, 117.0]  usask_1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>width</th>\n      <th>height</th>\n      <th>bbox</th>\n      <th>source</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>b6ab77fd7</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>[834.0, 222.0, 56.0, 36.0]</td>\n      <td>usask_1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>b6ab77fd7</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>[226.0, 548.0, 130.0, 58.0]</td>\n      <td>usask_1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>b6ab77fd7</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>[377.0, 504.0, 74.0, 160.0]</td>\n      <td>usask_1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>b6ab77fd7</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>[834.0, 95.0, 109.0, 107.0]</td>\n      <td>usask_1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>b6ab77fd7</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>[26.0, 144.0, 124.0, 117.0]</td>\n      <td>usask_1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# „Éá„Éº„Çø„Éï„É¨„Éº„É†„ÇíyoloÂΩ¢Âºè„Å´Â§âÊèõ\ndef convert_to_yolo(df: pd.DataFrame) -> pd.DataFrame:\n    bboxs = np.stack(df['bbox'].apply(lambda x: np.fromstring(x[1:-1], sep=',')))\n    for i, column in enumerate(['x', 'y', 'w', 'h']):\n        df[column] = bboxs[:,i]\n    df.drop(columns=['bbox'], inplace=True)\n    df['x_center'] = df['x'] + df['w']/2\n    df['y_center'] = df['y'] + df['h']/2\n    df['classes'] = 0\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-09-09T07:26:20.958309Z","iopub.execute_input":"2023-09-09T07:26:20.960850Z","iopub.status.idle":"2023-09-09T07:26:20.970720Z","shell.execute_reply.started":"2023-09-09T07:26:20.960806Z","shell.execute_reply":"2023-09-09T07:26:20.969763Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df = convert_to_yolo(df)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-09T07:26:20.972297Z","iopub.execute_input":"2023-09-09T07:26:20.973053Z","iopub.status.idle":"2023-09-09T07:26:22.052583Z","shell.execute_reply.started":"2023-09-09T07:26:20.973020Z","shell.execute_reply":"2023-09-09T07:26:22.051468Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"    image_id  width  height   source      x      y      w      h  x_center  \\\n0  b6ab77fd7   1024    1024  usask_1  834.0  222.0   56.0   36.0     862.0   \n1  b6ab77fd7   1024    1024  usask_1  226.0  548.0  130.0   58.0     291.0   \n2  b6ab77fd7   1024    1024  usask_1  377.0  504.0   74.0  160.0     414.0   \n3  b6ab77fd7   1024    1024  usask_1  834.0   95.0  109.0  107.0     888.5   \n4  b6ab77fd7   1024    1024  usask_1   26.0  144.0  124.0  117.0      88.0   \n\n   y_center  classes  \n0     240.0        0  \n1     577.0        0  \n2     584.0        0  \n3     148.5        0  \n4     202.5        0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>width</th>\n      <th>height</th>\n      <th>source</th>\n      <th>x</th>\n      <th>y</th>\n      <th>w</th>\n      <th>h</th>\n      <th>x_center</th>\n      <th>y_center</th>\n      <th>classes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>b6ab77fd7</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>usask_1</td>\n      <td>834.0</td>\n      <td>222.0</td>\n      <td>56.0</td>\n      <td>36.0</td>\n      <td>862.0</td>\n      <td>240.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>b6ab77fd7</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>usask_1</td>\n      <td>226.0</td>\n      <td>548.0</td>\n      <td>130.0</td>\n      <td>58.0</td>\n      <td>291.0</td>\n      <td>577.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>b6ab77fd7</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>usask_1</td>\n      <td>377.0</td>\n      <td>504.0</td>\n      <td>74.0</td>\n      <td>160.0</td>\n      <td>414.0</td>\n      <td>584.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>b6ab77fd7</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>usask_1</td>\n      <td>834.0</td>\n      <td>95.0</td>\n      <td>109.0</td>\n      <td>107.0</td>\n      <td>888.5</td>\n      <td>148.5</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>b6ab77fd7</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>usask_1</td>\n      <td>26.0</td>\n      <td>144.0</td>\n      <td>124.0</td>\n      <td>117.0</td>\n      <td>88.0</td>\n      <td>202.5</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# „Éá„Éº„Çø„ÇíÂàÜÂâ≤ \ndef stratified_kfold_split_df(df: pd.DataFrame) -> pd.DataFrame:\n    fold_id = np.zeros((df.shape[0],1))\n    skf = StratifiedKFold(n_splits = 5, random_state = 42, shuffle = True)\n    \n    # source„Åß„ÅÆÂ±§Âåñ\n    for (ff, (train_index, test_index)) in enumerate(skf.split(df, df['source'])):\n        fold_id[test_index]= int(ff)\n\n    df['fold'] = fold_id.copy()\n    df = df[['image_id','x', 'y', 'w', 'h','x_center','y_center','classes', 'fold']]\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-09-09T07:26:22.056045Z","iopub.execute_input":"2023-09-09T07:26:22.056975Z","iopub.status.idle":"2023-09-09T07:26:22.068626Z","shell.execute_reply.started":"2023-09-09T07:26:22.056938Z","shell.execute_reply":"2023-09-09T07:26:22.067541Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df = stratified_kfold_split_df(df)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-09T07:26:22.073332Z","iopub.execute_input":"2023-09-09T07:26:22.074036Z","iopub.status.idle":"2023-09-09T07:26:22.387340Z","shell.execute_reply.started":"2023-09-09T07:26:22.073998Z","shell.execute_reply":"2023-09-09T07:26:22.386249Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"    image_id      x      y      w      h  x_center  y_center  classes  fold\n0  b6ab77fd7  834.0  222.0   56.0   36.0     862.0     240.0        0   0.0\n1  b6ab77fd7  226.0  548.0  130.0   58.0     291.0     577.0        0   2.0\n2  b6ab77fd7  377.0  504.0   74.0  160.0     414.0     584.0        0   3.0\n3  b6ab77fd7  834.0   95.0  109.0  107.0     888.5     148.5        0   3.0\n4  b6ab77fd7   26.0  144.0  124.0  117.0      88.0     202.5        0   4.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>x</th>\n      <th>y</th>\n      <th>w</th>\n      <th>h</th>\n      <th>x_center</th>\n      <th>y_center</th>\n      <th>classes</th>\n      <th>fold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>b6ab77fd7</td>\n      <td>834.0</td>\n      <td>222.0</td>\n      <td>56.0</td>\n      <td>36.0</td>\n      <td>862.0</td>\n      <td>240.0</td>\n      <td>0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>b6ab77fd7</td>\n      <td>226.0</td>\n      <td>548.0</td>\n      <td>130.0</td>\n      <td>58.0</td>\n      <td>291.0</td>\n      <td>577.0</td>\n      <td>0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>b6ab77fd7</td>\n      <td>377.0</td>\n      <td>504.0</td>\n      <td>74.0</td>\n      <td>160.0</td>\n      <td>414.0</td>\n      <td>584.0</td>\n      <td>0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>b6ab77fd7</td>\n      <td>834.0</td>\n      <td>95.0</td>\n      <td>109.0</td>\n      <td>107.0</td>\n      <td>888.5</td>\n      <td>148.5</td>\n      <td>0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>b6ab77fd7</td>\n      <td>26.0</td>\n      <td>144.0</td>\n      <td>124.0</td>\n      <td>117.0</td>\n      <td>88.0</td>\n      <td>202.5</td>\n      <td>0</td>\n      <td>4.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def create_labels_and_copy_images(df: pd.DataFrame, fold: int, source: str = \"train\"):\n    val_index = set(df[df['fold'] == fold]['image_id'])\n\n    # ÁîªÂÉè„Åî„Å®„Å´„Éê„Ç¶„É≥„Éá„Ç£„É≥„Ç∞„Éú„ÉÉ„ÇØ„Çπ„Çí„É´„Éº„ÉóÂá¶ÁêÜ\n    for name,mini in tqdm(df.groupby('image_id')):    \n        # „Éï„Ç°„Ç§„É´„ÅÆ‰øùÂ≠òÂ†¥ÊâÄ\n        if name in val_index:\n            path2save = 'valid/'\n        else:\n            path2save = 'train/'   \n        # „É©„Éô„É´„ÅÆ„Çπ„Éà„É¨„Éº„Ç∏„Éë„Çπ\n        if not os.path.exists(f'convertor/fold{fold}/labels/' + path2save):\n            os.makedirs(f'convertor/fold{fold}/labels/' + path2save)\n        with open(f'convertor/fold{fold}/labels/' + path2save + name + \".txt\", 'w+') as f:\n            # YoloÂΩ¢Âºè„ÅÆË¶Å‰ª∂„Å´Âæì„Å£„Å¶Â∫ßÊ®ô„ÇíÊ≠£Ë¶èÂåñ\n            row = mini[['classes','x_center','y_center','w','h']].astype(float).values\n            row = row / 1024\n            row = row.astype(str)\n            for j in range(len(row)):\n                text = ' '.join(row[j])\n                f.write(text)\n                f.write(\"\\n\")\n        if not os.path.exists(f'convertor/fold{fold}/images/{path2save}'):\n            os.makedirs(f'convertor/fold{fold}/images/{path2save}')\n        # ÁîªÂÉè„ÅÆÂâçÂá¶ÁêÜ„ÅØ‰∏çË¶Å => „Åù„Çå„Çâ„Çí„Éê„ÉÉ„ÉÅ„Å®„Åó„Å¶„Ç≥„Éî„Éº\n        shutil.copy(f\"../input/global-wheat-detection/{source}/{name}.jpg\", f'convertor/fold{fold}/images/{path2save}/{name}.jpg')","metadata":{"execution":{"iopub.status.busy":"2023-09-09T07:26:22.392562Z","iopub.execute_input":"2023-09-09T07:26:22.394940Z","iopub.status.idle":"2023-09-09T07:26:22.410329Z","shell.execute_reply.started":"2023-09-09T07:26:22.394903Z","shell.execute_reply":"2023-09-09T07:26:22.408852Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"create_labels_and_copy_images(df, fold=2, source=\"train\")","metadata":{"execution":{"iopub.status.busy":"2023-09-09T07:26:22.411971Z","iopub.execute_input":"2023-09-09T07:26:22.412434Z","iopub.status.idle":"2023-09-09T07:26:52.333667Z","shell.execute_reply.started":"2023-09-09T07:26:22.412400Z","shell.execute_reply":"2023-09-09T07:26:52.332693Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3373/3373 [00:29<00:00, 113.43it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"!git clone https://github.com/ultralytics/yolov5  && cd yolov5 && pip install -r requirements.txt &> /dev/null","metadata":{"execution":{"iopub.status.busy":"2023-09-09T07:26:52.337726Z","iopub.execute_input":"2023-09-09T07:26:52.339339Z","iopub.status.idle":"2023-09-09T07:27:08.588661Z","shell.execute_reply.started":"2023-09-09T07:26:52.339298Z","shell.execute_reply":"2023-09-09T07:27:08.587489Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Cloning into 'yolov5'...\nremote: Enumerating objects: 15943, done.\u001b[K\nremote: Counting objects: 100% (63/63), done.\u001b[K\nremote: Compressing objects: 100% (47/47), done.\u001b[K\nremote: Total 15943 (delta 28), reused 39 (delta 16), pack-reused 15880\u001b[K\nReceiving objects: 100% (15943/15943), 14.61 MiB | 21.10 MiB/s, done.\nResolving deltas: 100% (10927/10927), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Ââ≤„ÇäÂΩì„Å¶„Çâ„Çå„ÅüGPU„ÇíÁ¢∫Ë™ç\ngpu_info = !nvidia-smi\ngpu_info = '\\n'.join(gpu_info)\nif gpu_info.find('failed') >= 0:\n    print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n    print('and then re-execute this cell.')\n    \nelse:\n    print(gpu_info)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T07:27:08.590853Z","iopub.execute_input":"2023-09-09T07:27:08.591222Z","iopub.status.idle":"2023-09-09T07:27:08.706845Z","shell.execute_reply.started":"2023-09-09T07:27:08.591184Z","shell.execute_reply":"2023-09-09T07:27:08.705771Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Sat Sep  9 07:27:08 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   32C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla T4            Off  | 00000000:00:05.0 Off |                    0 |\n| N/A   39C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# YAMLË®≠ÂÆö„Éï„Ç°„Ç§„É´","metadata":{}},{"cell_type":"code","source":"yaml_text = \"\"\"train: /kaggle/working/convertor/fold2/images/train/\nval: /kaggle/working/convertor/fold2/images/valid/\n\nnc: 1\nnames: ['wheat']\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-09-09T07:27:08.708215Z","iopub.execute_input":"2023-09-09T07:27:08.708843Z","iopub.status.idle":"2023-09-09T07:27:08.714603Z","shell.execute_reply.started":"2023-09-09T07:27:08.708810Z","shell.execute_reply":"2023-09-09T07:27:08.713501Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"with open(\"wheat.yaml\", 'w') as f:\n    f.write(yaml_text)\n%cat wheat.yaml","metadata":{"execution":{"iopub.status.busy":"2023-09-09T07:27:08.716061Z","iopub.execute_input":"2023-09-09T07:27:08.716765Z","iopub.status.idle":"2023-09-09T07:27:09.677698Z","shell.execute_reply.started":"2023-09-09T07:27:08.716679Z","shell.execute_reply":"2023-09-09T07:27:09.676622Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"train: /kaggle/working/convertor/fold2/images/train/\nval: /kaggle/working/convertor/fold2/images/valid/\n\nnc: 1\nnames: ['wheat']","output_type":"stream"}]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"!python ./yolov5/train.py --img 512 --batch 2 --epochs 3 --workers 2 --data wheat.yaml --cfg \"./yolov5/models/yolov5s.yaml\" --name yolov5x_fold2 --cache","metadata":{"execution":{"iopub.status.busy":"2023-09-09T07:27:09.681182Z","iopub.execute_input":"2023-09-09T07:27:09.681520Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"\u001b[34m\u001b[1mwandb\u001b[0m: WARNING ‚ö†Ô∏è wandb is deprecated and will be removed in a future release. See supported integrations at https://github.com/ultralytics/yolov5#integrations.\n\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: (30 second timeout) \n\u001b[34m\u001b[1mwandb\u001b[0m: W&B disabled due to login timeout.\n\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5/yolov5s.pt, cfg=./yolov5/models/yolov5s.yaml, data=wheat.yaml, hyp=yolov5/data/hyps/hyp.scratch-low.yaml, epochs=3, batch_size=2, imgsz=512, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=2, project=yolov5/runs/train, name=yolov5x_fold2, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ‚úÖ\nYOLOv5 üöÄ v7.0-215-ga6659d0 Python-3.10.12 torch-2.0.0 CUDA:0 (Tesla T4, 15110MiB)\n\n\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 üöÄ runs in Comet\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir yolov5/runs/train', view at http://localhost:6006/\nDownloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 755k/755k [00:00<00:00, 15.1MB/s]\nDownloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5/yolov5s.pt...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14.1M/14.1M [00:00<00:00, 123MB/s]\n\nOverriding model.yaml nc=80 with nc=1\n\n                 from  n    params  module                                  arguments                     \n  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\nYOLOv5s summary: 214 layers, 7022326 parameters, 7022326 gradients, 15.9 GFLOPs\n\nTransferred 342/349 items from yolov5/yolov5s.pt\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\nWARNING ‚ö†Ô∏è DP not recommended, use torch.distributed.run for best DDP Multi-GPU results.\nSee Multi-GPU Tutorial at https://docs.ultralytics.com/yolov5/tutorials/multi_gpu_training to get started.\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/working/convertor/fold2/labels/train... 48 images, 0 bac\u001b[0m\n\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /kaggle/working/convertor/fold2/labels/train.cache\n\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.0GB ram): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:00<00:00, 79.67it/\u001b[0m\n\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/convertor/fold2/labels/valid... 3325 images, 0 bac\u001b[0m\n\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /kaggle/working/convertor/fold2/labels/valid.cache\n\u001b[34m\u001b[1mval: \u001b[0mCaching images (2.4GB ram): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3325/3325 [00:45<00:00, 73.07i\u001b[0m\n\n\u001b[34m\u001b[1mAutoAnchor: \u001b[0m5.99 anchors/target, 0.997 Best Possible Recall (BPR). Current anchors are a good fit to dataset ‚úÖ\nPlotting labels to yolov5/runs/train/yolov5x_fold2/labels.jpg... \nImage sizes 512 train, 512 val\nUsing 1 dataloader workers\nLogging results to \u001b[1myolov5/runs/train/yolov5x_fold2\u001b[0m\nStarting training for 3 epochs...\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n        0/2     0.466G     0.1201    0.07523          0         59        512: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all       3325     147430    0.00594     0.0402    0.00318   0.000758\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n        1/2     0.466G     0.1174    0.04878          0         18        512: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all       3325     147430    0.00838     0.0567    0.00443    0.00103\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n        2/2     0.466G     0.1156     0.0605          0         12        512: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all       3325     147430    0.00988     0.0668     0.0053    0.00122\n\n3 epochs completed in 0.088 hours.\nOptimizer stripped from yolov5/runs/train/yolov5x_fold2/weights/last.pt, 14.4MB\nOptimizer stripped from yolov5/runs/train/yolov5x_fold2/weights/best.pt, 14.4MB\n\nValidating yolov5/runs/train/yolov5x_fold2/weights/best.pt...\nFusing layers... \nYOLOv5s summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n                 Class     Images  Instances          P          R      mAP50   ","output_type":"stream"}]},{"cell_type":"code","source":"!ls ./yolov5/runs/train/yolov5x_fold0/weights/ -lh","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"code","source":"!ls /kaggle/input/global-wheat-detection/test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python ./yolov5/detect.py --weights ./yolov5/runs/train/yolov5x_fold0/weights/best.pt --img 512 --conf 0.1 --source /kaggle/input/global-wheat-detection/test --save-txt --save-conf --exist-ok","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls ./yolov5/runs/detect/exp/labels/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_yolo_to_coco(s):\n    x = int(1024 * (s[1] - s[3]/2))\n    y = int(1024 * (s[2] - s[4]/2))\n    w = int(1024 * s[3])\n    h = int(1024 * s[4])\n    \n    return(str(s[5]) + ' ' + str(x) + ' ' + str(y) + ' ' + str(w) + ' ' + str(h))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('submission.csv', 'w') as myfile:\n\n    # prepare submission\n    wfolder = './yolov5/runs/detect/exp/labels/'\n    for f in os.listdir(wfolder):\n        fname = wfolder + f\n        xdat = pd.read_csv(fname, sep = ' ', header = None)\n        outline = f[:-4] + ' ' + ' '.join(list(xdat.apply(lambda s: convert_yolo_to_coco(s), axis = 1)))\n        myfile.write(outline + '\\n')\n        \nmyfile.close()   ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cat submission.csv","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}